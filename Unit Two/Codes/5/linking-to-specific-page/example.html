<html>

<head>
    <title>Example of source and destination</title>
</head>

<body>
    <div id="top">
        <h1>BIM Third Semester Courses</h1>
    </div>

    <ul>
        <a href="#web-tech">
            <li>IT 215 : Web Programming – I</li>
        </a><br>

        <a href="#fin-acc">
            <li>ACC 201: Financial Accounting</li>
        </a><br>

        <a href="#java">
            <li>IT 216: JAVA Programming – I</li>
        </a><br>

        <a href="#co">
            <li>IT 217 : Computer Organization</li>
        </a><br>

        <a href="#b-stat">
            <li>STT 201: Business Statistics</li>
        </a>
    </ul>

    <div id="web-tech">
        <h2><u>History of Web Technology</u></h2>
        <p>
            Tim Berners-Lee invented the World Wide Web in 1989, about 20 years after the first connection was
            established over what is today known as the Internet. At the time, Tim was a software engineer at CERN, the
            large particle physics laboratory near Geneva, Switzerland. Many scientists participated in experiments at
            CERN for extended periods of time, then returned to their laboratories around the world. These scientists
            were eager to exchange data and results, but had difficulties doing so. Tim understood this need, and
            understood the unrealized potential of millions of computers connected together through the Internet.Tim
            documented what was to become the World Wide Web with the submission of a proposal to his management at
            CERN, in late 1989 (see the proposal.), This proposal specified a set of technologies that would make the
            Internet truly accessible and useful to people. Believe it or not, Tim’s initial proposal was not
            immediately accepted. However, Tim persevered. By October of 1990, he had specified the three fundamental
            technologies that remain the foundation of today’s Web (and which you may have seen appear on parts of your
            Web browser):

            HTML: HyperText Markup Language. The publishing format for the Web, including the ability to format
            documents and link to other documents and resources.

            URI: Uniform Resource Identifier. A kind of “address” that is unique to each resource on the Web.

            HTTP: Hypertext Transfer Protocol. Allows for the retrieval of linked resources from across the Web.

            Tim also wrote the first Web page editor/browser (“WorldWideWeb”) and the first Web server (“httpd“). By the
            end of 1990, the first Web page was served. By 1991, people outside of CERN joined the new Web community.
            Very important to the growth of the Web, CERN announced in April 1993 that the World Wide Web technology
            would be available for anyone to use on a royalty-free basis.

            Since that time, the Web has changed the world. It has arguably become the most powerful communication
            medium the world has ever known. Whereas only 25% of the people on the planet are currently using the Web
            (and the Web Foundation aims to accelerate this growth substantially), the Web has changed the way we teach
            and learn, buy and sell, inform and are informed, agree and disagree, share and collaborate, meet and love,
            and tackle problems ranging from putting food on our tables to curing cancer.

            Tim Berners-Lee and others realized that for the Web to reach its full potential, the underlying
            technologies must become global standards, implemented in the same way around the world. Therefore, in 1994,
            Tim founded the World Wide Web Consortium (W3C) as a place for stakeholders to reach consensus around the
            specification and guidelines to ensure that the Web works for everyone and that it evolves in a responsible
            manner. W3C standards have enabled a single World Wide Web of information and people, and an
            increasingly-rich set of capabilities: Web 2.0 (personal and dynamic), Web 3.0 (a semantic Web of linked
            data), Web services, voice access, mobile access, accessibility for people with disabilities and for people
            speaking many languages, richer graphics and video, etc. The Web Foundation supports the work of W3C to
            ensure that the Web and the technologies that underpin it remain free and open to all.

            With over 1 trillion public pages (in 2008) and 1.7 billion people on the Web (in 2009), we do not really
            understand how these pieces work together and how to best improve the Web into the future. In 2005, Tim and
            colleagues started the Web Science Trust (WST). WST is building an international, multidisciplinary research
            community to examine the World Wide Web as “humanity connected by technology”. WST brings together computer
            scientists, sociologists, mathematicians, policy experts, entrepreneurs, decision makers and many others
            from around the world to better understand today’s Web and to develop solutions to guide the use and design
            of tomorrow’s Web. The Web Foundation believes the discipline of Web Science is critically important to
            advancing the Web, and supports WST‘s efforts to build and coordinate this new field of study.

            Most of the history of the Web is ahead of us. The Web is far from reaching its full potential as an agent
            of empowerment for everyone in the world. Web access through the world’s 4+ billion mobile phones is an
            incredible opportunity. New Web technologies will enable billions of people currently excluded from the Web
            community to join it. We must understand the Web and improve its capabilities. We must ensure that Web
            technologies are free and open for all to leverage. The work of the Web Foundation aims to have a
            substantial, positive impact on all of these factors, and on the future history of the Web.
        </p>
        <a href="#top">Back to top</a>
    </div>

    <div id="fin-acc">
        <h2><u>History of Financial Accounting</u></h2>
        <p>
            The history of accounting is fascinating and colorful. While today’s companies often rely on modern
            processes, the earliest records of business and financial transactions began on clay tablets. Today,
            automated technology is used in many accounts payable departments for data capture, validation, matching and
            more. Explore the compelling evolution of accounting and learn how the numbers game has become what it is
            now. Accounting in ancient history
            Accounting is a system of recording and summarizing financial and business transactions. Record keeping,
            accounting, and accounting tools have been used for as long as civilizations have engaged in trade. Many
            historians hypothesize that one reason writing systems were developed was to record trade transactions. Some
            of the earliest writings discovered by archaeologists are accounts of tax records on clay tablets. These
            first examples of accounting from Mesopotamia and Egypt date back to between 3300 and 2000 BCE.

            In the 13th century, medieval Europe moved toward a money economy. Merchants relied on bookkeeping to
            oversee the multiple simultaneous transactions financed by bank loans. As industry moved forward, it was
            clear that accounting procedures needed to be refined for higher accuracy and efficiency.

            The father of accounting
            In 1458, Benedetto Contrugli invented the double-entry accounting system - broadly defined as any
            bookkeeping system that involves a debit and/or credit entry for transactions. The double-entry system
            revolutionized accounting.

            Luca Bartolomes Pacioli, an Italian mathematician and Franciscan monk, invented a system of record keeping
            that used a memorandum, ledger, and journal. He also wrote many books on accounting. Paciolo became known as
            the father of accounting and bookkeeping. In 1494, he wrote Summa de Arithmetica, Geometria, Proportioni et
            Proportionalita ("The Collected Knowledge of Arithmetic, Geometry, Proportion, and Proportionality"), which
            included a 27-page treatise on bookkeeping.

            Centuries later, in 1854, the first professional organizations for accountants - the Edinburgh Society of
            Accountants and the Glasgow Institute of Accountants and Actuaries - were established in Scotland. Members
            of these early organizations could refer to themselves as “chartered accountants.” In 1887, the American
            Institute of Certified Public Accountants was established.

            The early systems and electronic data interchange
            Always finding ways to improve accounting and AP processes, inventors started to create machines for
            mathematical calculations. In the 1880s, American William Burroughs invented the adding machine. Early
            adding machines did not have the critical features of a computer, such as internal memory. However, they
            enabled accountants to carry out arithmetic with higher accuracy and efficiency.

            By the end of the century, technology continued to progress. Herman Hollerith developed a punch-card machine
            to speed up data handling for the U.S. Census. These innovative tabulating machines recorded data by
            punching a pattern of holes into cards. The machine could also read these patterns to call up pertinent
            information. Hollerith took the punch-card concept into private industry when he founded IBM. Businesses
            were using punch-card machines for accounting by 1907; an IBM tabulator could process 100 cards a minute by
            1928.

            Accounting took a large step toward the future  as we know it in 1955, when a company bought a computer for
            the first time purely for accounting. After World War II, General Electric purchased the UNIVAC (the
            UNIVersal Automatic Computer) to run payroll in its factories. Developed by John Mauchly and J. Presper
            Eckert, the UNIVAC stored data on magnetic tape instead of punch cards. The UNIVAC took 40 hours to complete
            the payroll process.

            The ongoing evolution of accounting technology
            AP processes were streamlined in the 1960s when the U.S. transportation industry developed EDI to
            standardize transactions between vendors and customers. By 1982, automotive industry customers including
            Ford and General Motors started to mandate EDI for their suppliers. Today, all major industries employ EDI,
            making it a significant milestone in the history of accounting. Sets of EDI standards, such as EDIFACT (the
            primary standard in Europe) and X12 (the primary standard in the United States), govern the structure and
            content of documents. 

            In 1978, the accounting world saw the birth of Visicalc. It was the first spreadsheet software that enabled
            financial modeling on the computer. During the same year, Peachtree Software introduced an accounting
            software package for the early personal computer. As a result, companies could computerize their accounting
            at a fraction of the cost of purchasing a mainframe. In 1998, Quickbooks was launched. It soon dominated the
            market for day-to-day bookkeeping. Today, over 4.5 million companies use Quickbooks, making it the most
            popular accounting program in the U.S.

            Optical character recognition (OCR) and intelligent data capture (IDC)
            Another shift in the history of accounting occurred in early 2000. The financial world experienced the entry
            of Advanced Data Capture: Optical Character Recognition (OCR) and Intelligent Data Capture (IDC) technology,
            which enables the automation of the accounts payable process. Instead of manually entering the information,
            key bits of data – such as the PO number, amount or date – are digitally captured and populated in a variety
            of index fields. Pre-set templates for commonly structured documents enable information to be automatically
            entered into the system. Customized business rules can be applied to add automation to the process from
            start to finish.

            Today companies are increasing their AP efficiency with benchmarks based on robust technology that
            identifies and tracks complex factors that impact the accounts payable workflow. Powerful performance
            dashboards and benchmarking capabilities improve the AP processes continuously. Analytics tools can monitor
            key performance indicators (KPIs) touchlessly, compare automation levels against real-world benchmarks,
            leverage best practice workflows, and consult experts for ongoing improvements.
        </p>
        <a href="#top">Back to top</a>
    </div>

    <div id="java">
        <h2><u>History of Java Programming</u></h2>
        <p>
            Java is an object-oriented programming language developed by James Gosling and colleagues at Sun
            Microsystems in the early 1990s. Unlike conventional languages which are generally designed either to be
            compiled to native (machine) code, or to be interpreted from source code at runtime, Java is intended to be
            compiled to a bytecode, which is then run (generally using JIT compilation) by a Java Virtual Machine. The
            language itself borrows much syntax from C and C++ but has a simpler object model and fewer low-level
            facilities. Java is only distantly related to JavaScript, though they have similar ids and share a C-like
            syntax. Java was started as a project called "Oak" by James Gosling in June 1991. Gosling's goals were to
            implement a virtual machine and a language that had a familiar C-like notation but with greater uniformity
            and simplicity than C/C++. The first public implementation was Java 1.0 in 1995. It made the promise of
            "Write Once, Run Anywhere", with free runtimes on popular platforms. It was fairly secure and its security
            was configurable, allowing for network and file access to be limited. The major web browsers soon
            incorporated it into their standard configurations in a secure "applet" configuration. popular quickly. New
            versions for large and small platforms (J2EE and J2ME) soon were designed with the advent of "Java 2". Sun
            has not announced any plans for a "Java 3". In 1997, Sun approached the ISO/IEC JTC1 standards body and
            later the Ecma International to formalize Java, but it soon withdrew from the process. Java remains a
            proprietary de facto standard that is controlled through the Java Community Process. Sun makes most of its
            Java implementations available without charge, with revenue being generated by specialized products such as
            the Java Enterprise System. Sun distinguishes between its Software Development Kit (SDK) and Runtime
            Environment (JRE) which is a subset of the SDK, the primary distinction being that in the JRE the compiler
            is not present.

            Platform independence

            The second characteristic, platform independence, means that programs written in the Java language must run
            similarly on diverse hardware. One should be able to write a program once and run it anywhere.

            This is achieved by most Java compilers by compiling the Java language code "halfway" to bytecode
            (specifically Java bytecode)—simplified machine instructions specific to the Java platform. The code is then
            run on a virtual machine (VM), a program written in native code on the host hardware that interprets and
            executes generic Java bytecode. Further, standardized libraries are provided to allow access to features of
            the host machines (such as graphics, threading and networking) in unified ways. Note that, although there's
            an explicit compiling stage, at some point, the Java bytecode is interpreted or converted to native machine
            instructions by the JIT compiler.

            There are also implementations of Java compilers that compile to native object code, such as GCJ, removing
            the intermediate bytecode stage, but the output of these compilers can only be run on a single architecture.

            Sun's license for Java insists that all implementations be "compatible". This resulted in a legal dispute
            with Microsoft after Sun claimed that the Microsoft implementation did not support the RMI and JNI
            interfaces and had added platform-specific features of their own. In response, Microsoft no longer ships
            Java with Windows, and in recent versions of Windows, Internet Explorer cannot support Java applets without
            a third-party plug-in. However, Sun and others have made available Java run-time systems at no cost for
            those and other versions of Windows.

            The first implementations of the language used an interpreted virtual machine to achieve portability. These
            implementations produced programs that ran more slowly than programs compiled to native executables, for
            instance written in C or C++, so the language suffered a reputation for poor performance. More recent JVM
            implementations produce programs that run significantly faster than before, using multiple techniques.

            The first technique is to simply compile directly into native code like a more traditional compiler,
            skipping bytecodes entirely. This achieves good performance, but at the expense of portability. Another
            technique, known as just-in-time compilation (JIT), translates the Java bytecodes into native code at the
            time that the program is run which results in a program that executes faster than interpreted code but also
            incurs compilation overhead during execution. More sophisticated VMs use dynamic recompilation, in which the
            VM can analyze the behavior of the running program and selectively recompile and optimize critical parts of
            the program. Dynamic recompilation can achieve optimizations superior to static compilation because the
            dynamic compiler can base optimizations on knowledge about the runtime environment and the set of loaded
            classes. JIT compilation and dynamic recompilation allow Java programs to take advantage of the speed of
            native code without losing portability.

            Portability is a technically difficult goal to achieve, and Java's success at that goal has been mixed.
            Although it is indeed possible to write programs for the Java platform that behave consistently across many
            host platforms, the large number of available platforms with small errors or inconsistencies led some to
            parody Sun's "Write once, run anywhere" slogan as "Write once, debug everywhere".

        </p>
        <a href="#top">Back to top</a>
    </div>

    <div id="co">
        <h2><u>History of Computer Organizations</u></h2>
        <p>
            In this section, we present a brief overview of computer history. Additional information can be found at The
            Digital Century link.

            In this historical epoch, computers were first developed by the Egyptians, who had the abacus and shadow
            clocks. In the pre-industrial era, mechanical calculators were developed by Pascal and Leibniz. During the
            Industrial Revolution, mechanical computers were envisioned, and parts of such machines were prototyped, by
            Charles Babbage. These computers were not constructed in their entirety, due to size, weight, and power
            requirements that could not be satisfied by the technology of the day. An interesting overview of early
            mechanical computers is given in this link.

            With the discovery of electricity, electronic tabulating machines were developed by Herman Hollerith, whose
            company was purchased by Thomas Watson, founder of the International Business Machines Corporation.
            Throughout the 1920s and 1930s, IBM marketed a variety of tabulating machines driven by electrified
            keyboards and having a variety of printers. Although unsophisticated, this type of hardware helped the
            business community become accustomed to the idea of machine-assisted inventory, payroll, and shipping.
            Additionally, the hardware developed by IBM was modified for use in its early computers. Thus, it could be
            said that the era of electro-mechanical tabulating machines in some ways prepared society for the advent of
            digital computers.

            The advent of World War II increased the demand for more accurate calculations. Rooms full of humans were
            employed in computing artillery trajectories, and the result was unacceptable error. A variety of computing
            research projects were undertaken at Princeton University, Harvard University, and the University of
            Pennsylvania. These resulted in room-size computers such as the Mark-I through Mark-IV, and the ENIAC, all
            of which used vacuum tubes. The vacuum tube machines were erroneous (tubes burned out or their response
            drifted frequently), power-intensive, slow (less than 10,000 integer multiplications per second), and hard
            to program, but provided a useful testbed for basic computer concepts.

            After WWII, the business community was slow to accept computers because of their cost, size, weight, power
            consumption, and the cost of maintaining them (including programmer salaries). However, the Defense
            Department funded computer research during the early years of the Cold War, from which resulted the second
            generation of computers. These machines used transistors instead of vacuum tubes, and were smaller, less
            power-consumptive, and easier to use. Business firms became more interested in computing, and IBM started to
            manufacture business and scientific computers (4000 and 7000 series, respectively).

            In the 1960s, transistors were integrated first on small circuit boards, then etched on wafers called
            integrated circuits. These were much smaller than the second-generation computer circuits, and predictably
            consumed less power, took up less space, and were easier to repair (or replace). In the 1960s, many
            electronics companies were in business that are no longer building digital computers today - General
            Electric, RCA, Honeywell, and Burroughs, to id but a few. IBM's System/360 was the first general-purpose
            computer to support both business and scientific calculations, and had a number of operating system features
            that were novel for its day, including upward compatibility of software, programmability of the operating
            system through a (dreadful) language called OS/JCL, as well as support for numerous programming languages.

            The 1970s saw the advent of much faster and more capable integrated circuits, which made computers smaller
            and faster. IBM's System/370 was the workhorse mainframe series of the era, but was challenged by similar
            architectures, such as those produced by the Ahmdahl Corporation. In the 1970s, two important trends
            developed in addition to mainframe computing. First, the supercomputer was developed largely due to the
            efforts of Seymour Cray, who pioneered high-performance computing in the 1960s with the CDC6600 that he
            developed for Control Data Corporation. Second, the minicomputer was developed by Digital Equipment
            Corporation (DEC), whose PDP series of machines was the first general-purpose computer that small
            universities or research laboratories could afford. A third trend that went almost unnoticed, was the
            gradual emergence of personal computers, which were initially the domain of hobbyists. From these early
            beginnings came the Apple-II, the world's first affordable, workable personal computer that could be
            operated in some ways like its larger ancestors (mainframe or the minicomputer).

            In the 1980s, integrated circuits gave way to very large scale integrated (VLSI) circuit technology, which
            eventually packed millions of transistors onto a single chip. This comprised the fourth generation of
            computing machine technology. As a result, personal computers became smaller and faster, posing a challenge
            to the minicomputer. The use of VLSI technology enabled companies like DEC to compete with the mainframe
            market by developing superminicomputers. On the personal computer side of the market, IBM introduced the
            IBM/PC in 1980, which revolutionized the desktop by providing a common, open architecture. A young fellow,
            who combined ideas from DEC's VMS operating system and the emerging UNIX operating system, headed a company
            that was chosen to write the first extensible PC operating system - MS-DOS. The rest, as they say, is
            history - Bill Gates and Microsoft rose with IBM and its processor developer Intel to become the dominant
            players in a multi-billion-dollar industry, which eventually eclipsed the mainframe market and consigned
            minicomputers, superminicomputers, and microcomputers to the dustbin of history (for all but the most highly
            customized applications).

            The 1990s saw the emergence of distributed or networked computing and the continued proliferation of
            personal computers. Another development of the 1990s was mobile computing, which could become the dominant
            paradigm for personal computing in the first decade of the new millenium. On the supercomputer front,
            massively parallel machines were developed to become more practical, easier to program, and small enough for
            a mid-sized university to purchase for research or teaching purposes. Parallel computer technology continues
            to grow, supported in part by ever-smaller integrated circuit components and more user-friendly software.
            Distributed technology continues to proliferate both in computing and communication. The Internet and
            World-Wide Web have unified these paradigms to become the dominant platforms for the computerized
            dissemination of knowledge.
        </p>
        <a href="#top">Back to top</a>
    </div>

    <div id="b-stat">
        <h2><u>History of Business Statistics</u></h2>
        <p>
            By the 18th century, the term "statistics" designated the systematic collection of demographic and economic
            data by states. For at least two millennia, these data were mainly tabulations of human and material
            resources that might be taxed or put to military use. In the early 19th century, collection intensified, and
            the meaning of "statistics" broadened to include the discipline concerned with the collection, summary, and
            analysis of data. Today, data is collected and statistics are computed and widely distributed in government,
            business, most of the sciences and sports, and even for many pastimes. Electronic computers have expedited
            more elaborate statistical computation even as they have facilitated the collection and aggregation of data.
            A single data analyst may have available a set of data-files with millions of records, each with dozens or
            hundreds of separate measurements. These were collected over time from computer activity (for example, a
            stock exchange) or from computerized sensors, point-of-sale registers, and so on. Computers then produce
            simple, accurate summaries, and allow more tedious analyses, such as those that require inverting a large
            matrix or perform hundreds of steps of iteration, that would never be attempted by hand. Faster computing
            has allowed statisticians to develop "computer-intensive" methods which may look at all permutations, or use
            randomization to look at 10,000 permutations of a problem, to estimate answers that are not easy to quantify
            by theory alone.

            The term "mathematical statistics" designates the mathematical theories of probability and statistical
            inference, which are used in statistical practice. The relation between statistics and probability theory
            developed rather late, however. In the 19th century, statistics increasingly used probability theory, whose
            initial results were found in the 17th and 18th centuries, particularly in the analysis of games of chance
            (gambling). By 1800, astronomy used probability models and statistical theories, particularly the method of
            least squares. Early probability theory and statistics was systematized in the 19th century and statistical
            reasoning and probability models were used by social scientists to advance the new sciences of experimental
            psychology and sociology, and by physical scientists in thermodynamics and statistical mechanics. The
            development of statistical reasoning was closely associated with the development of inductive logic and the
            scientific method, which are concerns that move statisticians away from the narrower area of mathematical
            statistics. Much of the theoretical work was readily available by the time computers were available to
            exploit them. By the 1970s, Johnson and Kotz produced a four-volume Compendium on Statistical Distributions
            (1st ed., 1969-1972), which is still an invaluable resource.

            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.

            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.

            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.

            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.

            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.
            Applied statistics can be regarded as not a field of mathematics but an autonomous mathematical science,
            like computer science and operations research. Unlike mathematics, statistics had its origins in public
            administration. Applications arose early in demography and economics; large areas of micro- and
            macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on
            learning from data and making best predictions, statistics also has been shaped by areas of academic
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.
            research including psychological testing, medicine and epidemiology. The ideas of statistical testing have
            considerable overlap with decision science. With its concerns with searching and effectively presenting
            data, statistics has overlap with information science and computer science.
        </p>
        <a href="#top">Back to top</a>
    </div>
</body>

</html>